# -*- coding: utf-8 -*-
"""SVHN_Testing_Model_main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SIeuS3WGGcJZ-ahBQrL-3oZL7yTiaphb

# How to Build
This Colab file uses the [SVHN dataset](http://ufldl.stanford.edu/housenumbers/) and builds a [convolutional neural network](https://towardsdatascience.com/convolutional-neural-networks-explained-9cc5188c4939) which helps to process a colored image whose information are arranged in a grid-like format. The goal is to correctly identify the number within a given image.

To start, connect the runtime click Runtime -> Change Runtime Type -> change the Hardware Accelerator to GPU and GPU Type to T4.

Then click Connect at the top right of the page.

Note: you may not be able to access the GPU due to usage variability. In this case, you can still run the model with CPU only, but the time it takes to train will be extended significantly.

Reminder: SVHN is for non-commercial use only. This notebook is solely meant to show an example on how to build and convert some model to TFLite and quantize it to fit onto a microcontroller.

The following files are required declarations (in order for the code to work properly). You MUST declare names for the following:


1.   FOLDER_TO_STORE_TF_MODELS
2.   FOLDER_OF_IMAGES
3.   NAME_OF_TF_MODEL
4.   NAME_OF_TF_QUANT_MODEL

In addition, you must create folders and add images (either in JPG or BMP) within FOLDER_OF_IMAGES.

To create a folder, click on the Folder Icon on the left side of your screen (located below the {x} icon) --> right click on the area within Files --> click New Folder --> rename folder to the name in FOLDER_OF_IMAGES.

To upload images to said folder, right click on the folder --> click Upload --> select all of the images you want to upload --> hit OK.

To upload the datasets from SVHN, click [here](http://ufldl.stanford.edu/housenumbers/) --> scroll down to Format 2 --> download the testing and training set. Then, upload them by right clicking in the Files area --> click Upload --> select both train_32x32.mat and test_32x32.mat --> hit OK.

Once everything above has been completed, go to Runtime --> Run All.
"""

import numpy as np
import pathlib
import keras
import tensorflow as tf
import tensorflow_datasets as tfds
from matplotlib import pyplot as plt
from scipy.io import loadmat
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelBinarizer
from keras.preprocessing import image
from keras.preprocessing.image import ImageDataGenerator

from os import listdir, mkdir
from os.path import isfile, join, exists

# DECLARATIONS
FOLDER_TO_STORE_TF_MODELS = 'tf_end_models'
FOLDER_OF_IMAGES = 'image_folder'
NAME_OF_TF_MODEL = 'svhn_test_model.tflite'
NAME_OF_TF_QUANT_MODEL = 'svhn_test_quant_model.tflite'
IMG_WIDTH, IMG_HEIGHT = 32, 32

# Commented out IPython magic to ensure Python compatibility.
# %pwd

def run_assertions():
  # Check the train and test SVHN datasets are here.
  assert len(FOLDER_TO_STORE_TF_MODELS) > 0
  assert len(FOLDER_OF_IMAGES) > 0
  assert len(NAME_OF_TF_MODEL) > 0
  assert len(NAME_OF_TF_QUANT_MODEL) > 0
  assert exists('./' + FOLDER_TO_STORE_TF_MODELS)
  assert exists('./' + FOLDER_OF_IMAGES)

run_assertions()

"""Upload the SVHN datasets to the local environment. You can import them [here](http://ufldl.stanford.edu/housenumbers/)."""

# Load the data
(train_ds, val_ds, test_ds), metadata = tfds.load('svhn_cropped',
                                                  split=['train[:80%]', 'train[80%:100%]', 'test'],
                                                  with_info=True,
                                                  as_supervised=True)

# Sanity check (check that the number of classes)
num_classes = metadata.features['label'].num_classes
print("Number of classes: ", num_classes)

# Use convention train_take_ds for tfds
# train_images, test_images
train_take_ds = tfds.as_numpy(train_ds)
val_take_ds = tfds.as_numpy(val_ds)
test_take_ds = tfds.as_numpy(test_ds)

def return_images_and_labels(dataset=None):
  assert dataset is not None
  dataset_images = np.empty((len(dataset), 32, 32, 3), dtype='float32')
  dataset_labels = np.empty((len(dataset), 1), dtype='int8')

  index = 0
  for image, label in dataset:
    dataset_images[index] = image
    dataset_labels[index] = label
    index += 1

  return (dataset_images, dataset_labels)

train_images, train_labels = return_images_and_labels(train_take_ds)
val_images, val_labels = return_images_and_labels(val_take_ds)
test_images, test_labels = return_images_and_labels(test_take_ds)

print(train_labels.shape)

"""# Preprocess the data
For the purposes of quantization, our images will have to be in *float32* format. We use [numpy's astype](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html) function to cast the entire numpy array to a float32.

[MATLAB](https://www.mathworks.com/help/matlab/creating_plots/image-types.html) states that color values represented as *double* range between 0 to 1, whereas color values represented as *uint8* range between 0 to 255. Floats must follow the same convention for *double*, so we divide the values by 255.
"""

# Normalize the images data
"""
Why do we normalize the images data? We want the photos to be on a similar
scale to each other. We've converted the images to a float number,
and they all range between 0 and 255.
"""
print('Min: {}, Max: {}'.format(train_images.min(), train_images.max()))

train_images /= 255.0
test_images /= 255.0

"""The labels are organized in this fashion: [1], [2], [3], ..., [0]. Each digit represents a [class](https://machinelearningmastery.com/types-of-classification-in-machine-learning/#:~:text=In%20machine%20learning%2C%20classification%20refers,one%20of%20the%20known%20characters.).

To make things simple for us, for each image, we should check if the digit is part of the class or not. So, we use [LabelBinarizer](https://github.com/scikit-learn/scikit-learn/blob/7f9bad99d/sklearn/preprocessing/_label.py#L168) (part of scikit-learn) during training to create a binary classifier for each class. For example, if a number is labelled 9, this is what it looks like:
`[0 0 0 0 0 0 0 0 1 0]`.
"""

"""
Ok, so for the labels, we have an integer representation for each of the
training and testing sets. One-hot encoding only seems to be used for
categorical variables (i.e., when we have labels "red", "blue", "green",
we should have some numbers assigned to them ==> "red" == 1, "blue" == 2,
"green" == 3).

We need to do this because the labels are classified in an array, and we want
to be able to clearly and distinctly identify them.

But for our purposes, we can do it just to see.
"""
lb = LabelBinarizer()
train_labels = lb.fit_transform(train_labels)
print("Train labels: ", lb.classes_)
val_labels = lb.fit_transform(val_labels)
print("Validation labels: ", lb.classes_)
test_labels = lb.fit_transform(test_labels)
print("Test labels: ", lb.classes_)

# TODO: generate data augmentation here
data_augmentation = keras.Sequential([
    keras.layers.RandomRotation([-2*np.pi, 2*np.pi])
])
print(train_labels)
print(test_labels)

"""# Defining the Model

When defining the model, it's important to note how this model looks like.

[Sequential models](https://keras.io/guides/sequential_model/) are constructed out of layers stacked on top of each other. Each layer accepts one input tensor, and returns one output tensor. The input and output tensor's shapes may not be the same.

For a convolutional neural network (CNN), there are generally four different layers we must include:

1.   [Convolutional 2D layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D). These are layers which are small relative to the width and height, but are important in identifying key features within a photo. These work by passing a n x m grid over a subset of the images' pixels, and performing element wise multiplication over the image, which are then summed together. The resulting matrix is called the *feature map*, which allows the model to observe certain features within a given image.
2.   [RELU layers](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html). RELU is the activation function y = max(0, x). For our sake, we already included this function within the Conv2D layer, but essentially we zero out any negative values to prevent summations to zero.
3.   [Max pooling layers.](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPooling2D) These are layers that reduce the dimensions of the feature maps coming out of the convolution layers; for max pooling, we select the maximum value within a relatively small region of the feature map.
4. [Fully connected/dense layers.](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) These are layers that connect the last convolution layer and gives the final prediction. Dense layers connect all of the neurons from the previous convolution layer to each neuron in its own layer.

Generally, CNN architectures will have a convolution layer, followed by a RELU layer and max pooling layer in that order. This format can be repeated, followed by a layer that flattens everything.

We add [Dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) layers in order to prevent overfitting, which set input units randomly to 0, and any non-zero input units are scaled up by 1/(1 - rate). This only happens during training. For our sake, in hidden layers (layers that are neither the first nor the last layer), we set the Dropout to a rate of 0.4-0.5 as a guide.

[Softmax functions](https://deepai.org/machine-learning-glossary-and-terms/softmax-layer) help to simulate predictions by turning a set of real values, to a set of real values that sum to 1. So, after passing an image through all of the hidden layers, we output a vector of probabilities and take the index whose probability is the highest. That provides the likely number within the image.

[Early stopping](https://keras.io/api/callbacks/early_stopping/) helps prevent the model from training further when there's no improvement on the loss function or when the model starts to overfit. In this case, after 10 epochs where the loss function does not improve, we stop training.

The goal during training is to minimize the amount of error or the loss function. Loss functions refer to the difference between how the algorithm is performing under its current output and expected output. [Optimizers such as Adam](https://keras.io/api/optimizers/adam/) help to adjust the neural network's weights during training in order for the network to learn quicker. We use [categorical_crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/categorical_crossentropy) as it's often used for multi-classification problems where there are multiple classes an input can fall under and when we use one-hot encoding.

"""

keras.backend.clear_session()

model = keras.Sequential([
    keras.layers.Conv2D(32, (3, 3), padding='same',
                           activation='relu',
                           input_shape=(32, 32, 3)),
    keras.layers.BatchNormalization(),
    keras.layers.Conv2D(32, (3, 3), padding='same',
                        activation='relu'),
    keras.layers.MaxPooling2D((2, 2)),
    keras.layers.Dropout(0.4),
    keras.layers.Conv2D(64, (3, 3), padding='same',
                           activation='relu'),
    keras.layers.BatchNormalization(),
    keras.layers.Conv2D(64, (3, 3), padding='same',
                           activation='relu'),
    keras.layers.MaxPooling2D((2, 2)),
    keras.layers.Dropout(0.4),
    keras.layers.Conv2D(128, (2, 2), padding='same',
                           activation='relu'),
    keras.layers.BatchNormalization(),
    keras.layers.MaxPooling2D((2, 2)),
    keras.layers.Dropout(0.4),

    keras.layers.Flatten(),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dropout(0.4),
    keras.layers.Dense(10,  activation='softmax')
])

early_stopping = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)
optimizer = keras.optimizers.Adam(learning_rate=1e-3, amsgrad=True)
model.compile(optimizer=optimizer,
              loss='categorical_crossentropy',
              metrics=['accuracy'])

"""Provides a summary of the model. We can check the number of parameters within the model."""

model.summary()

"""Train the model against the training set, and report the results. Sidenotes to remember about epochs and training:

1. To avoid overloading the computer's resources, we send the training images in batches to the model in each epoch. In this case, we set it to 128 images.
2. "[epochs" are **not** iterations](https://deepai.org/machine-learning-glossary-and-terms/epoch). Iterations refer to how many batches need to be sent through the model to complete one epoch, whereas one epoch is one cycle of fitting the model to the entire dataset.
3. Epochs are hyperparameters - as such, it's up to us to figure out how many epochs we want to run on the model. For us, we set the upper bound of epochs to 125, but it doesn't imply the model will go through all 125 epochs. There's no guarantee as to how many epochs will guarantee the best model - you will need to play around with the number.
"""

history = model.fit(train_images, train_labels, batch_size=128,
                    epochs=125,
                    validation_data=(val_images, val_labels),
                    callbacks=[early_stopping])

"""Show the history about the model's training process, including the loss, the validation and testing accuracy."""

history.history

"""Plotting the training and validation accuracy.

A key question is how to know if your model is [overfitted or underfitted](https://www.ibm.com/topics/overfitting#:~:text=Low%20error%20rates%20and%20a,error%20rate%2C%20it%20signals%20overfitting.)? Overfitting occurs when the model is "fitted" too closely to the training dataset. This suggests the model will fail to recognize images outside of the training dataset (i.e., images in the real world). Underfitting refers to the model not being habituated to general features within the training dataset, usually caused by not training the model enough.

Generally, you can detect overfitting if the loss function in the training set is considerably less than that in the testing set. Underfitting is shown when the loss function in the training set is considerably higher than that in the testing set. This is not a guarantee though, and you need to look at the graphs to see the training vs. validation accuracy and loss overtime.
"""

"""
Plotting the training and validation accuracy
"""

plt.figure(figsize=(20, 10))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='val')
plt.ylabel('loss')
plt.legend()
plt.title("Epochs vs. Training and Validation Loss")

plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='train')
plt.plot(history.history['val_accuracy'], label='val')
plt.ylabel('accuracy')
plt.legend()
plt.title("Epochs vs. Training and Validation Accuracy")
plt.show()

"""Evaluate the model against the testing images and testing labels."""

test_loss, test_acc = model.evaluate(x=test_images, y=test_labels, verbose=0)
print('Test accuracy is: {:0.4f} \nTest loss is: {:0.4f}'.format(test_acc, test_loss))

"""Converting the model to Tensorflow Lite and write the model to a folder we've already uploaded. This is important in order to run the model on a small device."""

# Convert the model to TFLite
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

# We already created the directory....
# time to optimize!
tflite_models_dir = pathlib.Path("./" + FOLDER_TO_STORE_TF_MODELS)
tflite_model_file = tflite_models_dir/NAME_OF_TF_MODEL
tflite_model_file.write_bytes(tflite_model)

"""If the original Tensorflow Lite model is too big (at the time of writing, the constraints were at most 512 kB), then we need to [quantize](https://www.tensorflow.org/lite/performance/post_training_integer_quant/). For microcontrollers, this requires us to do a few steps:

1.   Generate a representative dataset of the images from the training set.
2.   Convert the inference types to unsigned 8-bit integers.
3.   Perform optimizations

This is a simple example of how to [generate a representative dataset](https://www.tensorflow.org/lite/performance/post_training_integer_quant#convert_using_integer-only_quantization). We take 400 random images within the training set and cast them to a float32.

In the future, it would be more useful to consider implementing [signatures](https://www.tensorflow.org/lite/guide/signatures) for representative data generations. However, this is only available for Tensorflow 2.7 onwards.
"""

"""
Representative data generator (as a precursor to quantization).
When performing integer quantization, it's required for you to pick
out a set of data points from your training set that best represents
your training set.

For our purposes in this notebook, we will simply take 400 random images from
the training dataset (there are over 75000 photos).

There may be better ways to generate the dataset, but as you'll see,
the accuracy is not heavily impacted after quantization compared to
the original model.
"""
def representative_data_gen():
    for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(400):
        yield[tf.dtypes.cast(input_value, tf.float32)]

"""Quantize the model and write it to another tflite model."""

# Optimize the model
"""
For our purposes, we will use integer quantization, and use the
default signatures that Keras API leverages.
"""
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_data_gen
# If any operations can't be quantized, converter throws an error
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
# Set input and output tensors to uint8
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
tflite_model_quant = converter.convert()
tflite_model_quant_file = tflite_models_dir/NAME_OF_TF_QUANT_MODEL
tflite_model_quant_file.write_bytes(tflite_model_quant)

"""Test inferences for any image you may have."""

def pred_value(pred):
  num_index = np.argmax(pred)
  if num_index == 9:
    return 0
  return num_index + 1

"""Upload the image to the model directly, taking into account the width and height."""

def upload_image(file_name, width, height):
  assert file_name is not None
  img = tf.keras.utils.load_img(file_name, target_size=(width, height))

  x = tf.keras.utils.img_to_array(img)
  x = np.expand_dims(x, axis=0)

  images = np.vstack([x])
  predictions = model.predict(images)

  print("Predictions: ", predictions)
  print("Index: ", pred_value(predictions[0]))
  img.show()

"""Run all of the photos located in the target folder through the model."""

def run_photos(photo_path_name):
  global IMG_WIDTH, IMG_HEIGHT
  assert exists(photo_path_name)
  list_photos = [f for f in listdir(photo_path_name) if isfile(join(photo_path_name, f))]
  for list_photo in list_photos:
    list_photo_full = photo_path_name + "/" + list_photo
    print("Photo: ", list_photo_full)
    upload_image(list_photo_full, IMG_WIDTH, IMG_HEIGHT)

run_photos("./" + FOLDER_OF_IMAGES)

"""To simulate how a model would take in an input, we create an interpreter for the quantized model, and allocate tensors for the input and output types."""

# Loading the quantized model to an interpreter

interpreter_quant = tf.lite.Interpreter(model_content=tflite_model_quant)
input_type = interpreter_quant.get_input_details()[0]['dtype']
output_type = interpreter_quant.get_output_details()[0]['dtype']
interpreter_quant.allocate_tensors()

def send_to_quant(file_name, width, height):
  global input_type
  img = tf.keras.utils.load_img(file_name, target_size=(width, height))

  x = tf.keras.utils.img_to_array(img)
  x = np.expand_dims(x, axis=0)
  x = x.astype('uint8')
  print("Shape: ", x.shape)
  # images = np.vstack([x])

  print("Array: ", x)
  print("Input type: ", input_type)
  interpreter_quant.set_tensor(input_type, x)
  interpreter_quant.invoke()
  predictions = interpreter_quant.get_tensor(output_type)
  print("Predictions: ", predictions)
  print("Index: ", pred_value(predictions[0]))
  img.show()

print("Interpreter shape: ", interpreter_quant.get_input_details()[0])